{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, Mapping\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "S = str\n",
    "DataType = Sequence[Sequence[Tuple[S, float]]]\n",
    "ProbFunc = Mapping[S, Mapping[S, float]]\n",
    "RewardFunc = Mapping[S, float]\n",
    "ValueFunc = Mapping[S, float]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_return_samples(\n",
    "        data: DataType\n",
    ") -> Sequence[Tuple[S, float]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, return) pairs.\n",
    "    Note: (state, return) pairs is not same as (state, reward) pairs.\n",
    "    \"\"\"\n",
    "    return [(s, sum(r for (_, r) in l[i:]))\n",
    "            for l in data for i, (s, _) in enumerate(l)]\n",
    "\n",
    "\n",
    "def get_mc_value_function(\n",
    "        state_return_samples: Sequence[Tuple[S, float]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular MC Value Function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    rewards = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    for state, r in state_return_samples:\n",
    "        counts[state] += 1\n",
    "        rewards[state] += r\n",
    "    return {state: rewards[state]/counts[state] for state in rewards}\n",
    "\n",
    "\n",
    "def get_state_reward_next_state_samples(\n",
    "        data: DataType\n",
    ") -> Sequence[Tuple[S, float, S]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, reward, next_state) triples.\n",
    "    \"\"\"\n",
    "    return [(s, r, l[i + 1][0] if i < len(l) - 1 else 'T')\n",
    "            for l in data for i, (s, r) in enumerate(l)]\n",
    "\n",
    "\n",
    "def get_probability_and_reward_functions(\n",
    "        srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> Tuple[ProbFunc, RewardFunc]:\n",
    "    \"\"\"\n",
    "    Implement code that produces the probability transitions and the\n",
    "    reward function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    seqs_by_state = defaultdict(list)\n",
    "    for sample in srs_samples:\n",
    "        seqs_by_state[sample[0]].append(sample[1:])\n",
    "    prob_func = defaultdict(dict)\n",
    "    rewards = defaultdict(list)\n",
    "    rewards['T'] = 0\n",
    "    for state in seqs_by_state:\n",
    "        counts = defaultdict(int)\n",
    "        count = 0\n",
    "        for sample in seqs_by_state[state]:\n",
    "            counts[sample[1]] += 1\n",
    "            rewards[state].append(sample[0])\n",
    "            count += 1\n",
    "        prob_func[state] = {s2: counts[s2] / count for s2 in counts}\n",
    "    reward_func = {state: np.mean(rewards[state]) for state in rewards}\n",
    "    return prob_func, reward_func\n",
    "\n",
    "\n",
    "def get_mrp_value_function(\n",
    "        prob_func: ProbFunc,\n",
    "        reward_func: RewardFunc\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement code that calculates the MRP Value Function from the probability\n",
    "    transitions and reward function, compatible with the interface defined above.\n",
    "    Hint: Use the MRP Bellman Equation and simple linear algebra\n",
    "    \"\"\"\n",
    "\n",
    "    value_func = defaultdict(int)\n",
    "\n",
    "    states = reward_func.keys()\n",
    "    transitions = np.zeros((len(states), len(states)))\n",
    "    rewards = np.zeros(len(states))\n",
    "    for i, s in enumerate(states):\n",
    "        rewards[i] = reward_func[s]\n",
    "        for j, s2 in enumerate(states):\n",
    "            if s2 in prob_func[s]:\n",
    "                transitions[i, j] = prob_func[s][s2]\n",
    "\n",
    "    vf = np.linalg.inv(np.eye(len(states)) - transitions).dot(rewards)\n",
    "\n",
    "    for i, s in enumerate(states):\n",
    "        value_func[s] = vf[i]\n",
    "    return value_func\n",
    "\n",
    "\n",
    "def get_td_value_function(\n",
    "        srs_samples: Sequence[Tuple[S, float, S]],\n",
    "        num_updates: int = 300000,\n",
    "        learning_rate: float = 0.3,\n",
    "        learning_rate_decay: int = 30\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular TD(0) (with experience replay) Value Function compatible\n",
    "    with the interface defined above. Let the step size (alpha) be:\n",
    "    learning_rate * (updates / learning_rate_decay + 1) ** -0.5\n",
    "    so that Robbins-Monro condition is satisfied for the sequence of step sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = defaultdict(int)\n",
    "\n",
    "    for i in range(num_updates):\n",
    "        alpha = learning_rate * (i / learning_rate + 1) ** -0.5\n",
    "        if i % len(srs_samples) == 0:\n",
    "            np.random.shuffle(srs_samples)\n",
    "        state, reward, next_state = srs_samples[i % len(srs_samples)]\n",
    "        value_function[state] += alpha * (reward + value_function[next_state] - value_function[state])\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def get_lstd_value_function(\n",
    "        srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement LSTD Value Function compatible with the interface defined above.\n",
    "    Hint: Tabular is a special case of linear function approx where each feature\n",
    "    is an indicator variables for a corresponding state and each parameter is\n",
    "    the value function for the corresponding state.\n",
    "    \"\"\"\n",
    "    states = list(filter(lambda x: x != 'T', set([s[0] for s in srs_samples] + [s[2] for s in srs_samples])))\n",
    "    states_to_ind = {s: i for i, s in enumerate(states)}\n",
    "    features = np.eye(len(states))\n",
    "\n",
    "    sum_diffs = np.zeros((len(states), len(states)))\n",
    "    sum_rewards = np.zeros(len(states))\n",
    "    for s1, r1, s2 in srs_samples:\n",
    "        s1_ind = states_to_ind[s1]\n",
    "        sx1 = features[:, s1_ind].reshape((len(states), 1))\n",
    "        if s2 != 'T':\n",
    "            s2_ind = states_to_ind[s2]\n",
    "            sx2 = features[:, s2_ind].reshape((len(states), 1))\n",
    "        else:\n",
    "            sx2 = np.zeros(sx1.shape)\n",
    "        sum_diffs += sx1.dot((sx1 - sx2).T)\n",
    "        sum_rewards += features[:, s1_ind] * r1\n",
    "\n",
    "    w = np.linalg.inv(sum_diffs).dot(sum_rewards)\n",
    "    value_func = defaultdict(int)\n",
    "    for i, s in enumerate(states):\n",
    "        value_func[s] = w[i]\n",
    "    return value_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- MONTE CARLO VALUE FUNCTION --------------\n",
      "{'A': 9.571428571428571, 'B': 5.642857142857143}\n",
      "-------------- MDP VALUE FUNCTION ----------\n",
      "defaultdict(<class 'int'>, {'T': 0.0, 'A': 12.933333333333334, 'B': 9.6})\n",
      "------------- TD VALUE FUNCTION --------------\n",
      "defaultdict(<class 'int'>, {'B': 9.598455586887109, 'T': 0, 'A': 12.932235034225606})\n",
      "------------- LSTD VALUE FUNCTION --------------\n",
      "defaultdict(<class 'int'>, {'B': 9.600000000000001, 'A': 12.933333333333334})\n"
     ]
    }
   ],
   "source": [
    "given_data: DataType = [\n",
    "    [('A', 2.), ('A', 6.), ('B', 1.), ('B', 2.)],\n",
    "    [('A', 3.), ('B', 2.), ('A', 4.), ('B', 2.), ('B', 0.)],\n",
    "    [('B', 3.), ('B', 6.), ('A', 1.), ('B', 1.)],\n",
    "    [('A', 0.), ('B', 2.), ('A', 4.), ('B', 4.), ('B', 2.), ('B', 3.)],\n",
    "    [('B', 8.), ('B', 2.)]\n",
    "]\n",
    "\n",
    "sr_samps = get_state_return_samples(given_data)\n",
    "\n",
    "print(\"------------- MONTE CARLO VALUE FUNCTION --------------\")\n",
    "print(get_mc_value_function(sr_samps))\n",
    "\n",
    "srs_samps = get_state_reward_next_state_samples(given_data)\n",
    "\n",
    "pfunc, rfunc = get_probability_and_reward_functions(srs_samps)\n",
    "print(\"-------------- MDP VALUE FUNCTION ----------\")\n",
    "print(get_mrp_value_function(pfunc, rfunc))\n",
    "\n",
    "print(\"------------- TD VALUE FUNCTION --------------\")\n",
    "print(get_td_value_function(srs_samps))\n",
    "\n",
    "print(\"------------- LSTD VALUE FUNCTION --------------\")\n",
    "print(get_lstd_value_function(srs_samps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
